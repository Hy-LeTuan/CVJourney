{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST dataset\n",
    "\n",
    "The MNIST dataset contains hand-drawn images of digits from 0 - 9. The digits were digitally drawn on a black canvas with white paint.\n",
    "\n",
    "## Dataset structure\n",
    "\n",
    "The [MNIST](https://www.kaggle.com/datasets/scolianni/mnistasjpg) dataset contains a `trainingSet` with the images and the corresponding label for each image. The `testSet` contains unlabeled images that we could run inference on.\n",
    "\n",
    "# The image classifier\n",
    "\n",
    "## The image classifier input & output\n",
    "\n",
    "The image classifier will take in as input an image of a hand drawn image. Then it will analyze and give us a prediction on what it thinks the digit is.\n",
    "\n",
    "## The image classifier architecture\n",
    "\n",
    "The image classifier we are going to build is a Neural Network. Now the neural network will contain `linear layers` that will analyze the image and a `softmax layer` that will give us the prediction.\n",
    "\n",
    "## Training the image classifer\n",
    "\n",
    "To train our image classifier, we will need to feed our network images of the digits. The network will then give us a digit that it thinks the image represents. It is very likely that the network will answer wrong, because it hasn't learnt anything yet. To make it learn, we will punish it each time it makes a mistake and reward it each time it gives a correct answer. This punish and reward system will happen via a `loss function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/trainingSet/trainingSet/0/img_1.jpg\n"
     ]
    }
   ],
   "source": [
    "# python variables are named using snake_case where a space ' ' is represented by an underscore '_'\n",
    "\n",
    "# declaring a python variable with the = operator\n",
    "ds_root = \"./dataset/trainingSet/trainingSet\" # the path to the dataset. The '.' represents the current folder this file is in\n",
    "\n",
    "# the join function combines all the folders to create a path leading to the file\n",
    "image_path = os.path.join(ds_root, \"0\", \"img_1.jpg\") # this outputs: dataset/trainingSet/trainingSet/0/img_1.jpg\n",
    "\n",
    "# the print function is used to display\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7df31451c190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the image using matplotlib\n",
    "image =  plt.imread(image_path)\n",
    "\n",
    "# show the image also using matplotlib\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at this image, most of us would know that this is a zero. This is because we've seen so many zeros in our life that the classification happens subconciously and we don't even have to actively think about it. \n",
    "\n",
    "After training, the Neural Network will also have the ability to recognize that this is a 0 *(hopefully)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 images ---\n",
      "['./dataset/trainingSet/trainingSet/0/img_1.jpg', './dataset/trainingSet/trainingSet/0/img_10007.jpg', './dataset/trainingSet/trainingSet/0/img_10010.jpg', './dataset/trainingSet/trainingSet/0/img_10017.jpg', './dataset/trainingSet/trainingSet/0/img_10032.jpg', './dataset/trainingSet/trainingSet/0/img_10039.jpg', './dataset/trainingSet/trainingSet/0/img_10043.jpg', './dataset/trainingSet/trainingSet/0/img_10059.jpg', './dataset/trainingSet/trainingSet/0/img_10074.jpg', './dataset/trainingSet/trainingSet/0/img_10082.jpg']\n",
      "total image count\n",
      "42000\n"
     ]
    }
   ],
   "source": [
    "# next, we'll quantitize our dataset to see how much training data we have\n",
    "\n",
    "image_path_wild = os.path.join(ds_root, \"**\", \"*.jpg\") # the star '*' acts as a wildcard character. Meaning, any file that ends in .jpg will be selected\n",
    "\n",
    "all_images = sorted(glob(image_path_wild)) # get all image paths and sort them\n",
    "\n",
    "# show 10 first image path\n",
    "print('first 10 images ---')\n",
    "print(all_images[:10]) \n",
    "\n",
    "print('total image count')\n",
    "print(len(all_images)) # the len returns the length of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of image for digit: 0 is : 4132\n",
      "The number of image for digit: 1 is : 4684\n",
      "The number of image for digit: 2 is : 4177\n",
      "The number of image for digit: 3 is : 4351\n",
      "The number of image for digit: 4 is : 4072\n",
      "The number of image for digit: 5 is : 3795\n",
      "The number of image for digit: 6 is : 4137\n",
      "The number of image for digit: 7 is : 4401\n",
      "The number of image for digit: 8 is : 4063\n",
      "The number of image for digit: 9 is : 4188\n"
     ]
    }
   ],
   "source": [
    "# let's then see how many images each types of digits has\n",
    "\n",
    "# in python, to use a loop, we use the `for` keyword\n",
    "# the os.listdir function will list out all the top level files and folders in that directory\n",
    "for folder in sorted(os.listdir(ds_root)): \n",
    "    image_in_folder_path_list = os.path.join(ds_root, folder, \"*.jpg\") # wildcard to select all .jpg files like before\n",
    "    image_in_folder_list = glob(image_in_folder_path_list)\n",
    "\n",
    "    print(f\"The number of image for digit: {folder} is : {len(image_in_folder_list)}\") # f\"\" is a formatted string where you can insert variables in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the dataset\n",
    "\n",
    "After exploring the dataset, we now need to determine how much of it we are going to use to train the model, and how much of it we are going to use to test the model out on how its learning process is going.\n",
    "\n",
    "The usual ratio for splitting the dataset is 80% training data and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images for training:\n",
      "33600\n",
      "number of images for testing:\n",
      "8400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to split the images, we will gather all the image path\n",
    "all_images = sorted(glob(os.path.join(ds_root, \"**\", \"*.jpg\")))\n",
    "\n",
    "train_split, test_split = train_test_split(all_images, test_size=0.2) # this will split the dataset into 80% training data and 20% testing data\n",
    "\n",
    "print(\"number of images for training:\")\n",
    "print(len(train_split))\n",
    "\n",
    "print(\"number of images for testing:\")\n",
    "print(len(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data loaders\n",
    "\n",
    "We have successfully splitted the dataset into training data and testing data. Next, we need a way to get these images randomly at any arbitrary amount at a time so that we can feed them to the image classifier.\n",
    "\n",
    "To do that, we will explore the `DataLoader` and `Dataset` classes from `PyTorch` and how we can define functions and classes in Python.\n",
    " \n",
    "## Dataset\n",
    "\n",
    "The purpoes of the Dataset is to return an image and its label based on any index we give it. For example if i give it index = 1, it will return the image at position 1 and its corresponding label. \n",
    "\n",
    "More specifically, the image has to be an array of values so that the neural network can understand and compute, and the label has to be one hot encoded for the loss function.\n",
    "\n",
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, image_path): \n",
    "        \"\"\"\n",
    "        the init method is called when you first instantiate an instance of this class\n",
    "        arguments:\n",
    "        image_path: all the image path available in the dataset\n",
    "        \"\"\"\n",
    "        self.image_path = image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        this function returns the length of the dataset when the len() function is invoked\n",
    "        \"\"\"\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        this function will return the image and its label according to the index argument\n",
    "        argument:\n",
    "        index: the order of the iamge label pair of the dataset\n",
    "        \"\"\"\n",
    "        image_path = self.image_path[index] # the image path will have the form \"./dataset/trainingSet/trainingSet/0/img_1.jpg\"\n",
    "\n",
    "        # converts this into a numpy array and normalize its values in the range [0, 1] because the max value inside any image is 255\n",
    "        image_array = np.array(plt.imread(image_path))  / 255 \n",
    "        image_tensor = torch.from_numpy(image_array).float()\n",
    "\n",
    "        label = int(image_path.split(\"/\")[-2]) # get the folder of the path\n",
    "\n",
    "        # one_hot_label = torch.zeros(10, dtype=torch.long)\n",
    "        # one_hot_label[label] = 1\n",
    "\n",
    "        return [image_tensor, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 33600\n",
      "image array shape:  torch.Size([28, 28])\n",
      "image label:  4\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MnistDataset(image_path=train_split)\n",
    "test_dataset = MnistDataset(image_path=test_split)\n",
    "\n",
    "print(\"train dataset length:\", len(train_dataset))\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "\n",
    "print(\"image array shape: \", image.shape)\n",
    "print(\"image label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in each step out of loader:  8\n",
      "number of labels in each step out of loader:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"number of images in each step out of loader: \", len(images))\n",
    "print(\"number of labels in each step out of loader: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The model architecture includes a `Flatten` layer and 5 `Linear` layers, of which 4 are normal layers and the final one is the output layer.\n",
    "\n",
    "1. The `Flatten` layer is to flatten our image with *width* and *height* of `28px x 28px` to a flat array with size of $28 * 28 = 784$ numbers. \n",
    "2. The `Linear` layer is a matrix multiplication of the input and the output. The main idea of the linear is to transform the output into some matrix of different dimensions that the model can understand better. Many linear layers also exposes many different features of the image.\n",
    "3. The output `Linear` layer has the output of shape: $10$ which is the number of digits avaiable. This layer transform the model's understanding of the image into its choices. The $10$ numbers in the model output is how confidence the model is on which digit the image is supposed to be. For example, the output of the model is: `[1, 4, 2, 8, 7, 4, 1, 1, 2, 6]`, the model is most confident that the image is the number $3$ since the score at number $3$ is $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImageClassifer(nn.Module):\n",
    "    def __init__(self, image_width=28, image_height=28, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        flatten_dim = image_width * image_height\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.linear_1 = nn.Linear(in_features=flatten_dim, out_features=flatten_dim)\n",
    "        self.linear_2 = nn.Linear(in_features=flatten_dim, out_features=flatten_dim)\n",
    "        self.linear_3 = nn.Linear(in_features=flatten_dim, out_features=flatten_dim)\n",
    "        self.linear_4 = nn.Linear(in_features=flatten_dim, out_features=flatten_dim)\n",
    "\n",
    "        self.out = nn.Linear(in_features=flatten_dim, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = self.linear_4(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "model = ImageClassifer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "out = model(images)\n",
    "print(\"output shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "This is the training loop to train the model. It involves punishing the model through the `loss_fn` variable, which is the loss function. The model is then optimized based on the loss function with the `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model : ImageClassifer, loader : DataLoader, optimizer, loss_fn, device=torch.device(\"cuda\")):\n",
    "    model.train(True)\n",
    "\n",
    "    # training loop\n",
    "    running_loss = 0\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Step: 0 | Train loss: 0\")\n",
    "\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images)\n",
    "\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # pbar.set_description_str(f\"Step: {i} | Train loss: {loss.item()}\", refresh=True)\n",
    "\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def test_one_epoch(model : ImageClassifer, loader : DataLoader, loss_fn, device=torch.device(\"cuda\")):\n",
    "    model.eval()\n",
    "\n",
    "    # training loop\n",
    "    running_loss = 0\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Step: 0 | Test loss: 0\")\n",
    "\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # pbar.set_description_str(f\"Step: {i} | Test loss: {loss.item()}\")\n",
    "\n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 312.47it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 673.68it/s]\n",
      " 10%|█         | 1/10 [00:15<02:15, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 0 | Train loss: 0.4894833586653251 | Test loss: 0.3286713433230207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:14<00:00, 295.20it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 699.25it/s]\n",
      " 20%|██        | 2/10 [00:30<02:03, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 1 | Train loss: 0.30610389483604755 | Test loss: 0.3008631591343631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:14<00:00, 299.43it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 681.48it/s]\n",
      " 30%|███       | 3/10 [00:46<01:48, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 2 | Train loss: 0.2845332560031896 | Test loss: 0.2985700456982124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 302.77it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 741.23it/s]\n",
      " 40%|████      | 4/10 [01:01<01:32, 15.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 3 | Train loss: 0.27077998743436876 | Test loss: 0.2841345590672323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 306.43it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 694.85it/s]\n",
      " 50%|█████     | 5/10 [01:16<01:16, 15.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 4 | Train loss: 0.26315440417444774 | Test loss: 0.2792838783311613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 304.02it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 728.15it/s]\n",
      " 60%|██████    | 6/10 [01:32<01:01, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not improve on epoch: 5 | Train loss: 0.25569385992507226 | Test loss: 0.2885778454531516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 305.28it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 707.65it/s]\n",
      " 70%|███████   | 7/10 [01:47<00:45, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not improve on epoch: 6 | Train loss: 0.2520121196557773 | Test loss: 0.27996658817598863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 321.60it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 721.81it/s]\n",
      " 80%|████████  | 8/10 [02:01<00:30, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not improve on epoch: 7 | Train loss: 0.24635892742310672 | Test loss: 0.28326138426394515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 306.66it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 702.30it/s]\n",
      " 90%|█████████ | 9/10 [02:17<00:15, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model improved on epoch: 8 | Train loss: 0.2427122342001946 | Test loss: 0.2758943555925396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 0 | Train loss: 0: 100%|██████████| 4200/4200 [00:13<00:00, 321.80it/s]\n",
      "Step: 0 | Test loss: 0: 100%|██████████| 1050/1050 [00:01<00:00, 759.21it/s]\n",
      "100%|██████████| 10/10 [02:31<00:00, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not improve on epoch: 9 | Train loss: 0.24076792293796426 | Test loss: 0.277939048851175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define train variables\n",
    "epochs = 10\n",
    "\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = ImageClassifer(image_width=28, image_height=28, num_classes=10).to(device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "best_loss = 10000000\n",
    "\n",
    "print(\"START TRAINING\")\n",
    "print(\"---------\")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_epoch_loss = train_one_epoch(model=model, loader=train_loader, optimizer=optimizer, loss_fn=loss_fn, device=device)\n",
    "    test_epoch_loss = test_one_epoch(model=model, loader=test_loader, loss_fn=loss_fn, device=device)\n",
    "\n",
    "    if test_epoch_loss < best_loss:\n",
    "        print(f\"Model improved on epoch: {epoch} | Train loss: {train_epoch_loss} | Test loss: {test_epoch_loss}\")\n",
    "        best_loss = test_epoch_loss\n",
    "    else:\n",
    "        print(f\"Model did not improve on epoch: {epoch} | Train loss: {train_epoch_loss} | Test loss: {test_epoch_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
